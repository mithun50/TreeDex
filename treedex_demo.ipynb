{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreeDex: Tree-Based Document RAG Framework\n",
    "\n",
    "**Vectorless RAG** — index any document into a navigable tree structure, then retrieve relevant sections using any LLM.\n",
    "\n",
    "Supports: **Gemini, OpenAI, Claude, Groq, Together AI, Fireworks, vLLM, LM Studio, OpenRouter, Ollama** — any OpenAI-compatible endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymupdf tiktoken google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write TreeDex Modules\n",
    "\n",
    "The following cells write the `treedex/` package directly into the Colab runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nos.makedirs(\"treedex\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/pdf_parser.py\nimport fitz  # pymupdf\nimport tiktoken\n\n_enc = tiktoken.get_encoding(\"cl100k_base\")\n\n\ndef _count_tokens(text: str) -> int:\n    return len(_enc.encode(text))\n\n\ndef extract_pages(pdf_path: str) -> list[dict]:\n    \"\"\"Extract text from each page of a PDF.\n\n    Returns a list of dicts with page_num, text, and token_count.\n    \"\"\"\n    pages = []\n    with fitz.open(pdf_path) as doc:\n        for i, page in enumerate(doc):\n            text = page.get_text()\n            pages.append({\n                \"page_num\": i,\n                \"text\": text,\n                \"token_count\": _count_tokens(text),\n            })\n    return pages\n\n\ndef pages_to_tagged_text(pages: list[dict], start: int, end: int) -> str:\n    \"\"\"Combine pages[start:end+1] into a string with physical index tags.\"\"\"\n    parts = []\n    for page in pages[start : end + 1]:\n        n = page[\"page_num\"]\n        parts.append(f\"<physical_index_{n}>{page['text']}</physical_index_{n}>\")\n    return \"\\n\".join(parts)\n\n\ndef group_pages(\n    pages: list[dict], max_tokens: int = 20000, overlap: int = 1\n) -> list[str]:\n    \"\"\"Split pages into token-budget groups, each returned as tagged text.\"\"\"\n    total_tokens = sum(p[\"token_count\"] for p in pages)\n\n    if total_tokens <= max_tokens:\n        return [pages_to_tagged_text(pages, 0, len(pages) - 1)]\n\n    groups: list[str] = []\n    group_start = 0\n\n    while group_start < len(pages):\n        running = 0\n        group_end = group_start\n\n        while group_end < len(pages):\n            page_tokens = pages[group_end][\"token_count\"]\n            if running + page_tokens > max_tokens and group_end > group_start:\n                group_end -= 1\n                break\n            running += page_tokens\n            group_end += 1\n        else:\n            group_end = len(pages) - 1\n\n        group_end = min(group_end, len(pages) - 1)\n        groups.append(pages_to_tagged_text(pages, group_start, group_end))\n\n        if group_end >= len(pages) - 1:\n            break\n\n        next_start = group_end + 1 - overlap\n        group_start = max(next_start, group_start + 1)\n\n    return groups\n\n\ndef merge_pdfs(pdf_paths: list[str], output_path: str) -> str:\n    \"\"\"Merge multiple PDF files into one. Returns the output path.\"\"\"\n    merged = fitz.open()\n    for path in pdf_paths:\n        with fitz.open(path) as doc:\n            merged.insert_pdf(doc)\n    merged.save(output_path)\n    merged.close()\n    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/tree_builder.py\ndef list_to_tree(flat_list: list[dict]) -> list[dict]:\n    \"\"\"Convert a flat list with `structure` fields into a hierarchical tree.\"\"\"\n    nodes_by_structure = {}\n    roots = []\n\n    for item in flat_list:\n        node = {**item, \"nodes\": []}\n        structure = node[\"structure\"]\n        nodes_by_structure[structure] = node\n\n        parts = structure.rsplit(\".\", 1)\n        if len(parts) == 1:\n            roots.append(node)\n        else:\n            parent_structure = parts[0]\n            parent = nodes_by_structure.get(parent_structure)\n            if parent is not None:\n                parent[\"nodes\"].append(node)\n            else:\n                roots.append(node)\n\n    return roots\n\n\ndef _assign_ranges(nodes: list[dict], boundary_end: int):\n    \"\"\"Recursively assign start_index and end_index to nodes.\"\"\"\n    for i, node in enumerate(nodes):\n        node[\"start_index\"] = node.get(\"physical_index\", 0)\n\n        if i + 1 < len(nodes):\n            node[\"end_index\"] = nodes[i + 1].get(\"physical_index\", 0) - 1\n        else:\n            node[\"end_index\"] = boundary_end\n\n        if node.get(\"nodes\"):\n            _assign_ranges(node[\"nodes\"], node[\"end_index\"])\n\n\ndef assign_page_ranges(tree: list[dict], total_pages: int) -> list[dict]:\n    \"\"\"Set start_index and end_index on each node.\"\"\"\n    _assign_ranges(tree, total_pages - 1)\n    return tree\n\n\ndef assign_node_ids(tree: list[dict]) -> list[dict]:\n    \"\"\"DFS traversal, assigns sequential IDs: '0001', '0002', etc.\"\"\"\n    counter = [0]\n\n    def _walk(nodes):\n        for node in nodes:\n            counter[0] += 1\n            node[\"node_id\"] = f\"{counter[0]:04d}\"\n            _walk(node.get(\"nodes\", []))\n\n    _walk(tree)\n    return tree\n\n\ndef find_large_nodes(\n    tree: list[dict],\n    max_pages: int = 10,\n    max_tokens: int = 20000,\n    pages: list[dict] | None = None,\n) -> list[dict]:\n    \"\"\"Return nodes that exceed page or token thresholds.\"\"\"\n    large = []\n\n    def _walk(nodes):\n        for node in nodes:\n            start = node.get(\"start_index\", 0)\n            end = node.get(\"end_index\", 0)\n            page_count = end - start + 1\n\n            is_large = page_count > max_pages\n\n            if not is_large and pages is not None:\n                token_sum = sum(\n                    p[\"token_count\"]\n                    for p in pages\n                    if start <= p[\"page_num\"] <= end\n                )\n                is_large = token_sum > max_tokens\n\n            if is_large:\n                large.append(node)\n\n            _walk(node.get(\"nodes\", []))\n\n    _walk(tree)\n    return large\n\n\ndef embed_text_in_tree(tree: list[dict], pages: list[dict]) -> list[dict]:\n    \"\"\"Add `text` field to each node by concatenating page text for its range.\"\"\"\n\n    def _walk(nodes):\n        for node in nodes:\n            start = node.get(\"start_index\", 0)\n            end = node.get(\"end_index\", 0)\n            node[\"text\"] = \"\\n\".join(\n                p[\"text\"] for p in pages if start <= p[\"page_num\"] <= end\n            )\n            _walk(node.get(\"nodes\", []))\n\n    _walk(tree)\n    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/tree_utils.py\nimport copy\nimport json\nimport re\n\n\ndef create_node_mapping(tree: list[dict]) -> dict:\n    \"\"\"Flatten tree into {node_id: node_dict} for O(1) lookups.\"\"\"\n    mapping = {}\n\n    def _walk(nodes):\n        for node in nodes:\n            if \"node_id\" in node:\n                mapping[node[\"node_id\"]] = node\n            _walk(node.get(\"nodes\", []))\n\n    _walk(tree)\n    return mapping\n\n\ndef strip_text_from_tree(tree: list[dict]) -> list[dict]:\n    \"\"\"Return a deep copy of the tree with all `text` fields removed.\"\"\"\n    stripped = copy.deepcopy(tree)\n\n    def _strip(nodes):\n        for node in nodes:\n            node.pop(\"text\", None)\n            _strip(node.get(\"nodes\", []))\n\n    _strip(stripped)\n    return stripped\n\n\ndef collect_node_texts(node_ids: list[str], node_map: dict) -> str:\n    \"\"\"Gather and concatenate text from a list of node IDs.\"\"\"\n    parts = []\n    for nid in node_ids:\n        node = node_map.get(nid)\n        if node is None:\n            continue\n        title = node.get(\"title\", \"Untitled\")\n        structure = node.get(\"structure\", \"\")\n        text = node.get(\"text\", \"\")\n        header = f\"[{structure}: {title}]\" if structure else f\"[{title}]\"\n        parts.append(f\"{header}\\n{text}\")\n    return \"\\n\\n\".join(parts)\n\n\ndef count_nodes(tree: list[dict]) -> int:\n    \"\"\"Recursively count total nodes in the tree.\"\"\"\n    total = 0\n    for node in tree:\n        total += 1\n        total += count_nodes(node.get(\"nodes\", []))\n    return total\n\n\ndef get_leaf_nodes(tree: list[dict]) -> list[dict]:\n    \"\"\"Return all nodes with empty `nodes` list.\"\"\"\n    leaves = []\n\n    def _walk(nodes):\n        for node in nodes:\n            children = node.get(\"nodes\", [])\n            if not children:\n                leaves.append(node)\n            else:\n                _walk(children)\n\n    _walk(tree)\n    return leaves\n\n\ndef tree_to_flat_list(tree: list[dict]) -> list[dict]:\n    \"\"\"Flatten hierarchy back to a list in DFS order.\"\"\"\n    result = []\n\n    def _walk(nodes):\n        for node in nodes:\n            flat = {k: v for k, v in node.items() if k != \"nodes\"}\n            result.append(flat)\n            _walk(node.get(\"nodes\", []))\n\n    _walk(tree)\n    return result\n\n\ndef extract_json(text: str):\n    \"\"\"Robust JSON extraction from LLM responses.\"\"\"\n    try:\n        return json.loads(text)\n    except json.JSONDecodeError:\n        pass\n\n    match = re.search(r\"```(?:json)?\\s*\\n?(.*?)```\", text, re.DOTALL)\n    if match:\n        block = match.group(1).strip()\n        try:\n            return json.loads(block)\n        except json.JSONDecodeError:\n            cleaned = re.sub(r\",\\s*([}\\]])\", r\"\\1\", block)\n            try:\n                return json.loads(cleaned)\n            except json.JSONDecodeError:\n                pass\n\n    for start_char, end_char in [(\"{\", \"}\"), (\"[\", \"]\")]:\n        start = text.find(start_char)\n        if start == -1:\n            continue\n        depth = 0\n        for i in range(start, len(text)):\n            if text[i] == start_char:\n                depth += 1\n            elif text[i] == end_char:\n                depth -= 1\n                if depth == 0:\n                    candidate = text[start : i + 1]\n                    try:\n                        return json.loads(candidate)\n                    except json.JSONDecodeError:\n                        cleaned = re.sub(r\",\\s*([}\\]])\", r\"\\1\", candidate)\n                        try:\n                            return json.loads(cleaned)\n                        except json.JSONDecodeError:\n                            break\n\n    raise ValueError(f\"Could not extract JSON from text: {text[:200]}...\")\n\n\ndef print_tree(tree: list[dict], indent: int = 0):\n    \"\"\"Pretty-print tree structure for debugging.\"\"\"\n    prefix = \"  \" * indent\n    for node in tree:\n        node_id = node.get(\"node_id\", \"????\")\n        structure = node.get(\"structure\", \"\")\n        title = node.get(\"title\", \"Untitled\")\n        start = node.get(\"start_index\", \"?\")\n        end = node.get(\"end_index\", \"?\")\n        print(f\"{prefix}[{node_id}] {structure}: {title} (pages {start}-{end})\")\n        print_tree(node.get(\"nodes\", []), indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/loaders.py\n\"\"\"Document loaders for multiple file formats.\"\"\"\n\nimport os\nimport re\nfrom html.parser import HTMLParser\n\nimport tiktoken\n\n_enc = tiktoken.get_encoding(\"cl100k_base\")\n\n\ndef _count_tokens(text: str) -> int:\n    return len(_enc.encode(text))\n\n\ndef _text_to_pages(text: str, chars_per_page: int = 3000) -> list[dict]:\n    \"\"\"Split plain text into synthetic pages by character count.\"\"\"\n    pages = []\n    for i in range(0, len(text), chars_per_page):\n        chunk = text[i : i + chars_per_page]\n        pages.append({\n            \"page_num\": len(pages),\n            \"text\": chunk,\n            \"token_count\": _count_tokens(chunk),\n        })\n    return pages\n\n\nclass PDFLoader:\n    \"\"\"Load PDF files using PyMuPDF.\"\"\"\n\n    def load(self, path: str) -> list[dict]:\n        from treedex.pdf_parser import extract_pages\n        return extract_pages(path)\n\n\nclass TextLoader:\n    \"\"\"Load plain text or markdown files.\"\"\"\n\n    def __init__(self, chars_per_page: int = 3000):\n        self.chars_per_page = chars_per_page\n\n    def load(self, path: str) -> list[dict]:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            text = f.read()\n        return _text_to_pages(text, self.chars_per_page)\n\n\nclass _HTMLStripper(HTMLParser):\n    \"\"\"Simple HTML-to-text converter using stdlib.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._parts: list[str] = []\n        self._skip = False\n\n    def handle_starttag(self, tag, attrs):\n        if tag in (\"script\", \"style\"):\n            self._skip = True\n\n    def handle_endtag(self, tag):\n        if tag in (\"script\", \"style\"):\n            self._skip = False\n        if tag in (\"p\", \"div\", \"br\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"tr\"):\n            self._parts.append(\"\\n\")\n\n    def handle_data(self, data):\n        if not self._skip:\n            self._parts.append(data)\n\n    def get_text(self) -> str:\n        raw = \"\".join(self._parts)\n        return re.sub(r\"\\n{3,}\", \"\\n\\n\", raw).strip()\n\n\nclass HTMLLoader:\n    \"\"\"Load HTML files, stripping tags to plain text (stdlib only).\"\"\"\n\n    def __init__(self, chars_per_page: int = 3000):\n        self.chars_per_page = chars_per_page\n\n    def load(self, path: str) -> list[dict]:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            html = f.read()\n        stripper = _HTMLStripper()\n        stripper.feed(html)\n        text = stripper.get_text()\n        return _text_to_pages(text, self.chars_per_page)\n\n\nclass DOCXLoader:\n    \"\"\"Load DOCX files using python-docx.\"\"\"\n\n    def __init__(self, chars_per_page: int = 3000):\n        self.chars_per_page = chars_per_page\n\n    def load(self, path: str) -> list[dict]:\n        import docx\n\n        doc = docx.Document(path)\n        text = \"\\n\".join(p.text for p in doc.paragraphs)\n        return _text_to_pages(text, self.chars_per_page)\n\n\n_EXTENSION_MAP = {\n    \".pdf\": PDFLoader,\n    \".txt\": TextLoader,\n    \".md\": TextLoader,\n    \".html\": HTMLLoader,\n    \".htm\": HTMLLoader,\n    \".docx\": DOCXLoader,\n}\n\n\ndef auto_loader(path: str) -> list[dict]:\n    \"\"\"Auto-detect file format and load pages.\"\"\"\n    ext = os.path.splitext(path)[1].lower()\n    loader_cls = _EXTENSION_MAP.get(ext)\n    if loader_cls is None:\n        raise ValueError(\n            f\"Unsupported file extension '{ext}'. \"\n            f\"Supported: {', '.join(_EXTENSION_MAP)}\"\n        )\n    return loader_cls().load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/llm_backends.py\n\"\"\"LLM backends for TreeDex.\n\nNamed providers (Gemini, OpenAI, Claude) lazy-import their SDKs.\nOpenAICompatibleLLM and OllamaLLM use only stdlib (urllib).\n\"\"\"\n\nimport json\nimport urllib.request\nimport urllib.error\nfrom abc import ABC, abstractmethod\n\n\nclass BaseLLM(ABC):\n    \"\"\"Base class for all LLM backends.\"\"\"\n\n    @abstractmethod\n    def generate(self, prompt: str) -> str:\n        \"\"\"Send a prompt and return the generated text.\"\"\"\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}()\"\n\n\nclass GeminiLLM(BaseLLM):\n    \"\"\"Google Gemini via google-generativeai SDK.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gemini-2.0-flash\"):\n        self.api_key = api_key\n        self.model_name = model\n        self._client = None\n\n    def _get_client(self):\n        if self._client is None:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            self._client = genai.GenerativeModel(self.model_name)\n        return self._client\n\n    def generate(self, prompt: str) -> str:\n        model = self._get_client()\n        response = model.generate_content(prompt)\n        return response.text\n\n    def __repr__(self):\n        return f\"GeminiLLM(model={self.model_name!r})\"\n\n\nclass OpenAILLM(BaseLLM):\n    \"\"\"OpenAI via openai SDK.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4o\"):\n        self.api_key = api_key\n        self.model_name = model\n        self._client = None\n\n    def _get_client(self):\n        if self._client is None:\n            import openai\n\n            self._client = openai.OpenAI(api_key=self.api_key)\n        return self._client\n\n    def generate(self, prompt: str) -> str:\n        client = self._get_client()\n        response = client.chat.completions.create(\n            model=self.model_name,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return response.choices[0].message.content\n\n    def __repr__(self):\n        return f\"OpenAILLM(model={self.model_name!r})\"\n\n\nclass ClaudeLLM(BaseLLM):\n    \"\"\"Anthropic Claude via anthropic SDK.\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"claude-sonnet-4-20250514\"):\n        self.api_key = api_key\n        self.model_name = model\n        self._client = None\n\n    def _get_client(self):\n        if self._client is None:\n            import anthropic\n\n            self._client = anthropic.Anthropic(api_key=self.api_key)\n        return self._client\n\n    def generate(self, prompt: str) -> str:\n        client = self._get_client()\n        response = client.messages.create(\n            model=self.model_name,\n            max_tokens=4096,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return response.content[0].text\n\n    def __repr__(self):\n        return f\"ClaudeLLM(model={self.model_name!r})\"\n\n\nclass OpenAICompatibleLLM(BaseLLM):\n    \"\"\"Universal backend for any OpenAI-compatible API endpoint.\n\n    Works with: Groq, Together AI, Fireworks, vLLM, LM Studio,\n    OpenRouter, Ollama (OpenAI mode), and any other compatible service.\n\n    Uses only stdlib (urllib) — zero SDK dependencies.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        model: str,\n        api_key: str | None = None,\n        max_tokens: int = 4096,\n        temperature: float = 0.0,\n    ):\n        self.base_url = base_url.rstrip(\"/\")\n        self.model = model\n        self.api_key = api_key\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n\n    def generate(self, prompt: str) -> str:\n        url = f\"{self.base_url}/chat/completions\"\n\n        payload = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n        }\n\n        data = json.dumps(payload).encode(\"utf-8\")\n\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.api_key:\n            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n\n        req = urllib.request.Request(url, data=data, headers=headers, method=\"POST\")\n\n        try:\n            with urllib.request.urlopen(req, timeout=120) as resp:\n                body = json.loads(resp.read().decode(\"utf-8\"))\n        except urllib.error.HTTPError as e:\n            error_body = e.read().decode(\"utf-8\", errors=\"replace\")\n            raise RuntimeError(\n                f\"API request failed ({e.code}): {error_body}\"\n            ) from e\n\n        return body[\"choices\"][0][\"message\"][\"content\"]\n\n    def __repr__(self):\n        return f\"OpenAICompatibleLLM(base_url={self.base_url!r}, model={self.model!r})\"\n\n\nclass OllamaLLM(BaseLLM):\n    \"\"\"Ollama native backend using /api/generate endpoint.\n\n    Uses only stdlib (urllib) — zero SDK dependencies.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"llama3\",\n        base_url: str = \"http://localhost:11434\",\n    ):\n        self.model = model\n        self.base_url = base_url.rstrip(\"/\")\n\n    def generate(self, prompt: str) -> str:\n        url = f\"{self.base_url}/api/generate\"\n\n        payload = {\n            \"model\": self.model,\n            \"prompt\": prompt,\n            \"stream\": False,\n        }\n\n        data = json.dumps(payload).encode(\"utf-8\")\n        headers = {\"Content-Type\": \"application/json\"}\n\n        req = urllib.request.Request(url, data=data, headers=headers, method=\"POST\")\n\n        try:\n            with urllib.request.urlopen(req, timeout=120) as resp:\n                body = json.loads(resp.read().decode(\"utf-8\"))\n        except urllib.error.HTTPError as e:\n            error_body = e.read().decode(\"utf-8\", errors=\"replace\")\n            raise RuntimeError(\n                f\"Ollama request failed ({e.code}): {error_body}\"\n            ) from e\n\n        return body[\"response\"]\n\n    def __repr__(self):\n        return f\"OllamaLLM(model={self.model!r})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/prompts.py\n\"\"\"Prompt templates for structure extraction and retrieval.\"\"\"\n\nSTRUCTURE_EXTRACTION_PROMPT = \"\"\"\\\nYou are a document structure analyzer. Given the following document text with \\\nphysical page index tags, extract the hierarchical structure (table of contents).\n\nReturn a JSON list of objects, each with:\n- \"structure\": hierarchical numbering like \"1\", \"1.1\", \"1.2.3\"\n- \"title\": the section/chapter title\n- \"physical_index\": the page number (from the <physical_index_N> tag) where this section starts\n\nRules:\n- Use the physical_index tags to determine page numbers\n- Create a logical hierarchy: chapters -> sections -> subsections\n- Every section must have a unique structure ID\n- Return ONLY valid JSON — no extra text\n\nDocument text:\n{text}\n\nJSON output:\n\"\"\"\n\nSTRUCTURE_CONTINUE_PROMPT = \"\"\"\\\nYou are continuing to extract the hierarchical structure of a document.\n\nHere is the structure extracted so far:\n{previous_structure}\n\nNow extract the structure from the next portion of the document. \\\nContinue the numbering from where the previous structure left off. \\\nIf a section from the previous portion continues into this portion, \\\ndo NOT duplicate it.\n\nReturn a JSON list of NEW sections only (same format as before).\n\nDocument text:\n{text}\n\nJSON output:\n\"\"\"\n\nRETRIEVAL_PROMPT = \"\"\"\\\nYou are a document retrieval system. Given a document's tree structure and a \\\nuser query, select the most relevant sections that would contain the answer.\n\nDocument structure:\n{tree_structure}\n\nUser query: {query}\n\nReturn a JSON object with:\n- \"node_ids\": list of node IDs (strings like \"0001\", \"0005\") that are most \\\nrelevant to the query\n- \"reasoning\": brief explanation of why these sections were selected\n\nSelect the smallest set of sections that fully covers the answer. \\\nPrefer leaf nodes over parent nodes when the leaf contains the specific content. \\\nReturn ONLY valid JSON.\n\nJSON output:\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/core.py\n\"\"\"TreeDex: Tree-based document RAG framework.\"\"\"\n\nimport json\nimport os\n\nfrom treedex.loaders import auto_loader, PDFLoader\nfrom treedex.pdf_parser import group_pages\nfrom treedex.tree_builder import (\n    assign_node_ids,\n    assign_page_ranges,\n    embed_text_in_tree,\n    find_large_nodes,\n    list_to_tree,\n)\nfrom treedex.tree_utils import (\n    collect_node_texts,\n    count_nodes,\n    create_node_mapping,\n    extract_json,\n    get_leaf_nodes,\n    print_tree,\n    strip_text_from_tree,\n)\nfrom treedex.prompts import (\n    STRUCTURE_EXTRACTION_PROMPT,\n    STRUCTURE_CONTINUE_PROMPT,\n    RETRIEVAL_PROMPT,\n)\n\n\nclass QueryResult:\n    \"\"\"Result of a TreeDex query.\"\"\"\n\n    def __init__(self, context: str, node_ids: list[str],\n                 page_ranges: list, reasoning: str):\n        self.context = context\n        self.node_ids = node_ids\n        self.page_ranges = page_ranges\n        self.reasoning = reasoning\n\n    @property\n    def pages_str(self) -> str:\n        \"\"\"Human-readable page ranges like 'pages 5-8, 12-15'.\"\"\"\n        if not self.page_ranges:\n            return \"no pages\"\n        parts = []\n        for start, end in self.page_ranges:\n            if start == end:\n                parts.append(str(start + 1))\n            else:\n                parts.append(f\"{start + 1}-{end + 1}\")\n        return \"pages \" + \", \".join(parts)\n\n    def __repr__(self):\n        return (\n            f\"QueryResult(nodes={self.node_ids}, {self.pages_str}, \"\n            f\"context_len={len(self.context)})\"\n        )\n\n\nclass TreeDex:\n    \"\"\"Tree-based document index for RAG retrieval.\"\"\"\n\n    def __init__(self, tree: list[dict], pages: list[dict],\n                 llm=None):\n        self.tree = tree\n        self.pages = pages\n        self.llm = llm\n        self._node_map = create_node_mapping(tree)\n\n    @classmethod\n    def from_file(cls, path: str, llm, loader=None,\n                  max_tokens: int = 20000, overlap: int = 1,\n                  verbose: bool = True):\n        \"\"\"Build a TreeDex index from a file.\"\"\"\n        if verbose:\n            print(f\"Loading: {os.path.basename(path)}\")\n\n        if loader is not None:\n            pages = loader.load(path)\n        else:\n            pages = auto_loader(path)\n\n        if verbose:\n            total_tokens = sum(p[\"token_count\"] for p in pages)\n            print(f\"  {len(pages)} pages, {total_tokens:,} tokens\")\n\n        return cls.from_pages(pages, llm, max_tokens=max_tokens,\n                              overlap=overlap, verbose=verbose)\n\n    @classmethod\n    def from_pages(cls, pages: list[dict], llm,\n                   max_tokens: int = 20000, overlap: int = 1,\n                   verbose: bool = True):\n        \"\"\"Build a TreeDex index from pre-extracted pages.\"\"\"\n        groups = group_pages(pages, max_tokens=max_tokens, overlap=overlap)\n\n        if verbose:\n            print(f\"  {len(groups)} page group(s) for structure extraction\")\n\n        all_sections = []\n        for i, group_text in enumerate(groups):\n            if verbose:\n                print(f\"  Extracting structure from group {i + 1}/{len(groups)}...\")\n\n            if i == 0:\n                prompt = STRUCTURE_EXTRACTION_PROMPT.format(text=group_text)\n            else:\n                prev_json = json.dumps(all_sections, indent=2)\n                prompt = STRUCTURE_CONTINUE_PROMPT.format(\n                    previous_structure=prev_json, text=group_text\n                )\n\n            response = llm.generate(prompt)\n            sections = extract_json(response)\n\n            if isinstance(sections, list):\n                all_sections.extend(sections)\n            elif isinstance(sections, dict) and \"sections\" in sections:\n                all_sections.extend(sections[\"sections\"])\n\n        if verbose:\n            print(f\"  Extracted {len(all_sections)} sections\")\n\n        tree = list_to_tree(all_sections)\n        assign_page_ranges(tree, total_pages=len(pages))\n        assign_node_ids(tree)\n        embed_text_in_tree(tree, pages)\n\n        if verbose:\n            print(f\"  Tree: {count_nodes(tree)} nodes\")\n\n        return cls(tree, pages, llm)\n\n    @classmethod\n    def from_tree(cls, tree: list[dict], pages: list[dict], llm=None):\n        \"\"\"Create a TreeDex from an existing tree and pages.\"\"\"\n        return cls(tree, pages, llm)\n\n    def query(self, question: str, llm=None) -> QueryResult:\n        \"\"\"Query the index and return relevant context.\"\"\"\n        active_llm = llm or self.llm\n        if active_llm is None:\n            raise ValueError(\"No LLM provided. Pass llm= to query() or TreeDex constructor.\")\n\n        stripped = strip_text_from_tree(self.tree)\n        tree_json = json.dumps(stripped, indent=2)\n\n        prompt = RETRIEVAL_PROMPT.format(\n            tree_structure=tree_json, query=question\n        )\n\n        response = active_llm.generate(prompt)\n        result = extract_json(response)\n\n        node_ids = result.get(\"node_ids\", [])\n        reasoning = result.get(\"reasoning\", \"\")\n\n        context = collect_node_texts(node_ids, self._node_map)\n\n        page_ranges = []\n        for nid in node_ids:\n            node = self._node_map.get(nid)\n            if node:\n                start = node.get(\"start_index\", 0)\n                end = node.get(\"end_index\", 0)\n                page_ranges.append((start, end))\n\n        return QueryResult(\n            context=context,\n            node_ids=node_ids,\n            page_ranges=page_ranges,\n            reasoning=reasoning,\n        )\n\n    def save(self, path: str) -> str:\n        \"\"\"Save the index to a JSON file.\"\"\"\n        stripped = strip_text_from_tree(self.tree)\n\n        data = {\n            \"version\": \"1.0\",\n            \"framework\": \"TreeDex\",\n            \"tree\": stripped,\n            \"pages\": self.pages,\n        }\n\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2)\n\n        return path\n\n    @classmethod\n    def load(cls, path: str, llm=None):\n        \"\"\"Load a TreeDex index from a JSON file.\"\"\"\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        tree = data[\"tree\"]\n        pages = data[\"pages\"]\n\n        assign_page_ranges(tree, total_pages=len(pages))\n        embed_text_in_tree(tree, pages)\n\n        return cls(tree, pages, llm)\n\n    def show_tree(self):\n        \"\"\"Pretty-print the tree structure.\"\"\"\n        print_tree(self.tree)\n\n    def stats(self) -> dict:\n        \"\"\"Return index statistics.\"\"\"\n        total_tokens = sum(p[\"token_count\"] for p in self.pages)\n        leaves = get_leaf_nodes(self.tree)\n        return {\n            \"total_pages\": len(self.pages),\n            \"total_tokens\": total_tokens,\n            \"total_nodes\": count_nodes(self.tree),\n            \"leaf_nodes\": len(leaves),\n            \"root_sections\": len(self.tree),\n        }\n\n    def find_large_sections(self, max_pages: int = 10,\n                            max_tokens: int = 20000) -> list[dict]:\n        \"\"\"Find sections that exceed size thresholds.\"\"\"\n        return find_large_nodes(\n            self.tree, max_pages=max_pages,\n            max_tokens=max_tokens, pages=self.pages\n        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile treedex/__init__.py\n\"\"\"TreeDex: Tree-based document RAG framework.\"\"\"\n\nfrom treedex.core import TreeDex, QueryResult\nfrom treedex.loaders import PDFLoader, TextLoader, HTMLLoader, DOCXLoader, auto_loader\nfrom treedex.llm_backends import (\n    GeminiLLM,\n    OpenAILLM,\n    ClaudeLLM,\n    OllamaLLM,\n    OpenAICompatibleLLM,\n)\n\n__version__ = \"0.1.0\"\n\n__all__ = [\n    \"TreeDex\",\n    \"QueryResult\",\n    \"PDFLoader\",\n    \"TextLoader\",\n    \"HTMLLoader\",\n    \"DOCXLoader\",\n    \"auto_loader\",\n    \"GeminiLLM\",\n    \"OpenAILLM\",\n    \"ClaudeLLM\",\n    \"OllamaLLM\",\n    \"OpenAICompatibleLLM\",\n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Setup LLM\n",
    "\n",
    "Choose your LLM backend. Examples below for **Gemini** (default) and **OpenAI-compatible** endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treedex import TreeDex, GeminiLLM, OpenAICompatibleLLM\n\n# --- Option A: Gemini (default) ---\nfrom google.colab import userdata\nGEMINI_KEY = userdata.get(\"GEMINI_API_KEY\")\nllm = GeminiLLM(api_key=GEMINI_KEY)\n\n# --- Option B: Groq via OpenAI-compatible ---\n# llm = OpenAICompatibleLLM(\n#     base_url=\"https://api.groq.com/openai/v1\",\n#     api_key=\"gsk_...\",\n#     model=\"llama-3.3-70b-versatile\"\n# )\n\n# --- Option C: Together AI ---\n# llm = OpenAICompatibleLLM(\n#     base_url=\"https://api.together.xyz/v1\",\n#     api_key=\"...\",\n#     model=\"meta-llama/Llama-3-70b-chat-hf\"\n# )\n\n# --- Option D: Local Ollama ---\n# from treedex import OllamaLLM\n# llm = OllamaLLM(model=\"llama3\")\n\nprint(f\"Using: {llm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload & Index a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files as colab_files\nuploaded = colab_files.upload()\npdf_name = list(uploaded.keys())[0]\nprint(f\"Uploaded: {pdf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the index\nindex = TreeDex.from_file(pdf_name, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tree structure\nindex.show_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View stats\nstats = index.stats()\nfor k, v in stats.items():\n    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\nindex.save(\"my_index.json\")\nprint(\"Saved to my_index.json\")\n\n# Load\nindex2 = TreeDex.load(\"my_index.json\", llm=llm)\nprint(f\"Loaded: {index2.stats()['total_nodes']} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\nresult = index.query(\"What are the main topics covered in this document?\")\n\nprint(f\"Relevant nodes: {result.node_ids}\")\nprint(f\"Source: {result.pages_str}\")\nprint(f\"Reasoning: {result.reasoning}\")\nprint(f\"\\nContext ({len(result.context)} chars):\")\nprint(result.context[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another query\nresult2 = index.query(\"Explain the key concepts in the first chapter.\")\n\nprint(f\"Nodes: {result2.node_ids}\")\nprint(f\"Source: {result2.pages_str}\")\nprint(f\"Reasoning: {result2.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Swap LLM Provider\n",
    "\n",
    "Demonstrate using a different LLM for queries (e.g., Groq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap to Groq for fast inference (uncomment and add your key)\n# groq_llm = OpenAICompatibleLLM(\n#     base_url=\"https://api.groq.com/openai/v1\",\n#     api_key=\"gsk_YOUR_KEY_HERE\",\n#     model=\"llama-3.3-70b-versatile\"\n# )\n#\n# # Query with the new LLM — same index, different brain\n# result3 = index.query(\"Summarize the introduction.\", llm=groq_llm)\n# print(f\"Groq result: {result3.node_ids}\")\n# print(f\"Reasoning: {result3.reasoning}\")\n\nprint(\"Uncomment the code above and add your Groq API key to try it!\")"
   ]
  }
 ]
}
